{
    "collab_server" : "",
    "contents" : "---\noutput: pdf_document\n---\n \n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, comment = \"> \", fig.height = 3, fig.align = \"center\")\n\n# Packages\nlibrary(grid)\nlibrary(coin)\nlibrary(boot)\nlibrary(simpleboot)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(AICcmodavg)\n#library(tidyr)\nlibrary(likert)\n \n# Data\n\n# Basic example data set\nperson = c('A','B','C','D','E','F')\n\n# Original \nyear1 = c(5,4,4,4,4,4)\nyear2 = c(2,5,5,5,5,4)\nyear3 = c(3,5,5,5,5,3)\nyear4 = c(1,5,5,5,5,5)\n\n# A more obvious version\n# year1 = c(3,3,3,3,3,3)\n# year2 = c(4,4,4,2,2,2)\n# year3 = c(5,4,3,3,2,1)\n# year4 = c(5,5,5,1,1,1)\n \nex_1 = data.frame(person, year1, year2, year3, year4)\n \nex_1_long = reshape2::melt(ex_1)\n\n# Larger example data set\nset.seed(29)\nmd = data.frame(Group = as.character(\"MD\"), Response1 = ordered(sample(1:5, 100, replace=T, prob=c(.1,.1,.1,.2,.5))), Response2 = ordered(sample(1:5, 100, replace=T, prob=c(.1,.3,.3,.25,.15))))\nrn = data.frame(Group = as.character(\"RN\"), Response1 = ordered(sample(1:5, 100, replace=T, prob=c(.1,.1,.5,.2,.1))), Response2 = ordered(sample(1:5, 100, replace=T, prob=c(.1,.15,.45,.15,.15))))\n \nboth = rbind(md, rn)\n \nmake_NAs = sample(1:200, 15, replace=F)\nboth$Response1[make_NAs] = NA\n \nmake_NAs2 = sample(1:200, 15, replace=F)\nboth$Response2[make_NAs2] = NA\n \nnames(both) = c(\"EmployeeType\", \"My team works well together.\", \"I have the tools I need to do my job.\")\n\n# Save for revision\n# likert_levels = c('Strongly Disagree', 'Disagree', 'Neither', 'Agree', 'Strongly Agree')\n```\n \n```{r logo}\n# logo must be in same dir as Rmd\nknitr::include_graphics(\"scLOGO_smH_3col_rgb.png\")\n```\n\n\\begin{center} \\fontsize{16}{36}\\selectfont \\textbf{Do not use averages with Likert scale data} \\end{center} \n\\  \n\n### Seattle Children's Hospital \\newline \\textit{Analytic Guidelines Series}\n \n**Dwight Barry, PhD**  \n*Enterprise Analytics*  \n \n*`r format(Sys.Date(), \"%d %B %Y\")`*   \n   \n\\  \n\n\n# Summary\n \n- Likert and similar ordinal-level scales have a variety of uses, particularly within surveys such as Family Experience, Culture of Safety, and Employee Engagement. They also occur in clinical care, for example, in the use of pain scores.  \n \n- When evaluated improperly---particularly through the use of averages---the results can be strikingly misleading. Obviously, misleading results could drive or promote action where none is warranted, and vice versa.  \n \n- In nearly all cases, not only is it mathematically wrong, **taking the average of a Likert-scale variable will *not* provide useful answers** to the questions end-users can use to make actionable decisions. In essence, the use of averages cannot account for the importance of capturing and understanding variabililty. Analysts should strive to avoid their use in any reporting solution or analytic product that uses ordinal-scale data.  \n \n- Better ways to represent ordinal-value results include histograms of the values themselves, the use of well-supported \"top-box\"-type proportions, and/or bar charts of percentage by score or score category (e.g., favorable/neutral/unfavorable).  \n\n- \"Statistical significance\" on changes or differences between response groups' medians or distribution shift  can be assessed through non-parametric frequentist tests (permutation, Mann-Whitney-Wilcoxon), Information Theory, or Bayesian analysis. *t*-tests should never be used on Likert scales because ordinal data does not meet the assumptions of a *t*-test (and when using frequentist tools, one must *also* account for multiple testing to reduce the chance of false positives).  \n \n- A good way to remember not to use means on Likert scale data is to think: The average of *Agree* and *Strongly Agree* is **not** *Agree-And-A-Half*.  \n\n*Note: all of the data in this document is fake, created specifically to illustrate particular points.*  \n\n\\newpage \n\n# Discussion\n\n## A simple example \n \nTake a simple example where a group of 6 people people take the same survey for 4 years, and the mean results for an important question, such as \"my team works well together\", are as follows: \n \nTaking the mean of these results gives you this:\n \n| Year 1 | Year 2 | Year 3 | Year 4 | \n|:------:|:------:|:------:|:------:|\n| `r round(mean(year1), 2)` | `r round(mean(year2), 2)` | `r round(mean(year3), 2)` | `r round(mean(year4), 2)` |\n \nFrom these values, one might conclude that there is an improvement from year 1 to year 2, and no change year-over-year after year 2. \n \nThe values that created the above results are as follows:  \n \n```{r ex_1_table}\nkable(ex_1, col.names = c(\"Individual\", \"Year 1\", \"Year 2\", \"Year 3\", \"Year 4\"))\n```\n \nYou might already see how management decisions would be made differently based on whether one had just the means or had the complete data. \n \nHowever, in the latter case, you risk reducing or eliminating anonymity, which is essential to get respondents to answer truthfully (not to mention being unethical). Further, poring over tables of answers for many people for long surveys makes that approach practically infeasible. Visualizing the results in ways that capture a more complete story provides an answer to both issues, as well as providing decision-makers with truly actionable information.\n\n \n \n\n## Visualizing Likert-scale data\n\n \n\n### Histograms\n \nHistograms of the actual score values are the best way to visualize Likert data---they have two real axes, showing counts by score value or category, so you can parse the visual and understand the results very quickly. Using the same data as above, you can instantly see that the \"improvement\" in year 2 was perhaps not an improvement after all: while most respondents appear to be satisfied above what they thought in year 1, one respondent may be at risk of leaving.        \n\n\\newpage  \n\n*Figure 1. Histogram of example Likert scale data.*  \n \n```{r histos, fig.height=4, fig.width=4}\nggplot(ex_1_long, aes(value)) +\n    geom_histogram(binwidth=1) +\n    facet_wrap(~variable, ncol=1) +\n    xlab(\"Likert Scale Value\") +\n    theme_bw()\n```\n \n\n\n### Likert charts\n \nThe main disadvantage of histograms is space; Likert charts---which are in essence just stacked bar charts---are far more compact. The disadvantage is that it takes slightly longer for a user to parse them, but when faced with lots of questions or groupings, they tend to be a better option.  \n\nThere are two kinds of Likert charts---those that use a center line for a point of reference, and those that do not, in which case they are simply percentage bar charts for individual questions or are mosaic plots when comparing groupings. In the graphs below, each score value has its own color, and each score category---e.g., unfavorable is 1-2, neutral is 3, and favorable is 4-5 on a 5-point scale---is summarized by a percentage value at the left, middle/interior, and right sides of the bar, respectively.    \n\n*Figure 2. Centered Likert chart.*  \n \n```{r ex_1_likert, fig.height = 2.5}\nex_1[2:5] = lapply(ex_1[2:5], factor, levels = 1:5)\nex_1_likert = likert(ex_1[2:5])\nplot(ex_1_likert, ordered = FALSE, group.order = names(ex_1[2:5]))\n```\n\n \n\n*Figure 3. Uncentered Likert chart (aka percent bar chart).*  \n\n```{r ex_1_likert_percent, fig.height = 2.5}\nplot(ex_1_likert, ordered = FALSE, centered = FALSE, group.order=names(ex_1[2:5]))\n```\n\n\\  \n\nNeither Likert chart type is as clear as the histogram at making the results immediately understandable, but again, histograms take more space, and busy decision makers often need to see the forest (all the questions) at the expense of some trees (each question). In this case, analysts might use the histograms to explore potentially important results themselves, and then use Likert charts in a report with some strategically-placed text highlighting important patterns they found with the histograms.  \n\n\n\n\n## How many respondents are enough?\n \nIt's common to think: \"We surveyed everyone in this department, therefore the results we see must be correct.\" However, how people responded to surveys depends on many factors---such as mood the date the survey is taken, recent events in life and in work, changes in organizational structure, and any number of other factors---and many internal surveys are given only once a year. Thus, survey results are really a *sample* of attitudes and opinions, subject to random events and natural fluctuations. \n \nTypical practice at SCH is to expose summary results for groups with six or more people. While this helps preserve some anonymity, it does not include enough responses to ensure the overall response is stable. Comparisons over time or across groups that are not based on stable results can lead to  conclusions about differences that may or may not reflect reality.   \n\nIn this context, *stable* means that the data accurately represent true changes (or lack of change) in the question at hand. It's basically impossible to distinguish natural variation from real change when you have small numbers of respondents. As a result, the National Center for Health Statistics, for example, does not publish results with less than 20 distinct cases or responses. \n\nThe relative standard error (RSE) is the metric used to evaluate whether you have enough values for the results to be stable. The standard error is an estimate of the likely difference between the results and the true value (which in surveys, even of complete populations, can't be known exactly due to the reasons mentioned above). The *relative* standard error is the standard error expressed as a percent of the measure or number of responses, which is a constant function: $\\frac{1}{\\sqrt{responses}} * 100$. This function can be seen in the graph on the next page.    \n\nGenerally, you want RSE values less than 20-25% to have some confidence that your results are stable.  \n\n\\newpage  \n\n*Figure 4. The RSE-response count function. The RSE associated with the use of 6 responses is marked with dark red, and the response count associated with an RSE of 25% is marked with dark blue.*  \n\n```{r rse, fig.width=8, fig.height=4.5}\nx = seq(1:50)\nrse = data.frame(x = x, y = (1 / sqrt(x)) * 100)\n\nggplot(rse, aes(x = x, y = y)) +\n  geom_line() +\n  geom_segment(aes(x=6, y=0, xend=6, yend=41), color=\"darkred\", arrow = arrow(length = unit(0.25, \"cm\"))) +\n  geom_segment(aes(x=6, y=41, xend=0, yend=41), color=\"darkred\", arrow = arrow(length = unit(0.25, \"cm\"))) +\n  geom_label(aes(x=6, y=-5), label = \"6\") +\n  geom_label(aes(x=-1.75, y=41), label = \"41\") +\n  geom_segment(aes(x=16, y=25, xend=16, yend=0), color=\"darkblue\", arrow = arrow(length = unit(0.25, \"cm\"))) +\n  geom_segment(aes(x=0, y=25, xend=16, yend=25), color=\"darkblue\", arrow = arrow(length = unit(0.25, \"cm\"))) +\n  geom_label(aes(x=16, y=-5), label = \"16\") +\n  geom_label(aes(x=-1.75, y=25), label = \"25\") +\n  xlab(\"Number of responses\") +\n  ylab(\"Relative Standard Error\")\n\n```\n\n\n \n \n## Is there a significant difference?  \n \nMany decision-makers want to know if a result is \"significantly different\" from, say, the same response from a previous time period, or between a couple of subgroups in the same response. Unfortunately, this is mostly useless, for two reasons. \n\nFirst, acting as if Likert or other ordinal scales are continuous level data leads to many problems of interpretation (see the [*Appendix*](#Appendix) for a summary table of measurement scales and appropriate statistics). There has been controversy over this distinction for many decades; however, a great way to understand the conceptual problem is to realize that the mean of *Agree* and *Strongly Agree* is **not** *Agree-And-A-Half*---it just makes no sense.  \n\nA subsequent argument might be that, no, it's not conceptually accurate, but it provides a sense for directional changes. However, such results still run into problems of interpretation: if you go from 4.16 to 4.33, have you gone from *Agree.16* to *Agree.33*? What does such an \"improvement\" mean, in practical terms? All you can accurately say is that both values are most consistent with an *Agree* opinion. \n\nSpecifically in the medicine/healthcare context, [Kuzon et al.](https://www.ncbi.nlm.nih.gov/pubmed/8883724) state that the use of parametric statistics on ordinal data (such calculating a mean or using a *t*-test) is the first of \"The seven deadly sins of statistical analysis\". Don't \"sin\" and you don't have to worry about whether your results are illegitmate.    \n\nThere are a few ways around this: 1) use medians or other quantiles and test for differences in those statistics (these differences are best assessed via bootstrap or permutation testing), 2) test whether the distribution has shifted (Mann-Whitney-Wilcoxon test), or 3) use more advanced techniques such as multinomial or proportional-odds regression (see the [*Advanced*](#Advanced) section, below). These options are the more statistically-correct ways to do it (as opposed a *t*-test).  \n\nSo, using the simple example above, we might want to know whether the median is statistically different between year 1 (Median = `r median(year1)`) and year 2 (Median = `r median(year2)`). Running a [permutation test](https://en.wikipedia.org/wiki/Resampling_(statistics)#Permutation_tests) gives us the following results:   \n\n\\newpage   \n\n```{r permtest}\n# Subset to years 1 and 2\nex_1_long_y12 = dplyr::filter(ex_1_long, variable == \"year1\" | variable == \"year2\")\n\n# Permutation test\ncoin::oneway_test(value ~ variable, data = ex_1_long_y12, distribution = \"exact\")\n```\n\n\\ \n\nWhile our effect size is \"1\"---more accurately, *Agree* to *Strongly Agree*---the *p*-value of the test is very large (basically 1), so we cannot say that this difference is \"statistically significant\". \n\nWe could also ask, \"has the distribution shifted?\", which would involve using the [Mann-Whitney-Wilcoxon test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test):  \n\n```{r cmon_mann}\nwilcox.test(value ~ variable, data = ex_1_long_y12) \n```\n\n\\ \n\nThe *p*-value is non-significant, so the difference between year 1 and year 2 can't be assumed to be a statistically significant change.  \n\nLooking at the raw data or graphs seen earlier, a decision-maker might be justified in wanting to act, but the analysis suggests that the difference is not statistically significant.  \n\nThis leads us to the second problem with using *p*-values for determining whether a statistically-significant difference has occurred: sample size.  \n\n*p*-values are directly dependent on sample size. If your sample is large enough, you are guaranteed to have a small *p*-value. If your sample is small, whether or not you get a significant *p*-value depends on the scale of difference between the groups, i.e., the effect size.  \n\nFor example, consider the following examples evaluating the number of people who answer *Agree* or *Strongly Agree* (the \"favorable\" score group) to a question:  \n\n| Example | Favorable | Total Answers | Effect size | *p*-value |\n| ---------- | --------:| --------:|:-------------------:|:---------:|\n| 1 | 15 | 20 | 75% | 0.04 |\n| 2 | 114 | 200 | 57% | 0.04 |\n| 3 | 1,046 | 2,000 | 52% | 0.04 |\n| 4 | 1,001,450 | 2,000,000 | 50% | 0.04 | \n\nWith 15 of 20 people selecting a favorable value on the Likert scale, we have an effect size of 75%, which is certainly an effect worth taking seriously. That value is also a statistically significant difference (*p* < 0.05), which supports the idea that the majority has a favorable opinion. With a couple of thousand responses (example 3), we again have a statistically significant difference, but the effect size is now only 52%, close enough to even-preference as to be *practically* the same. In medical terms, we might think of this as statistically significant but clinically irrelevant.  \n\nFor these reasons---[and many others outside the scope of these guidelines](http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)---statisticians are moving away from the use of *p*-values. In frequentist statistics, these are being replaced by the use of effect sizes and confidence intervals (CIs); these provide information on both on the precision of the estimated difference, as well as whether the difference can be considered statistically distinct. If the CI includes 0, the difference is not-significant. Regardless of the location of 0, the width of the CI tells you how precise your estimate is.    \n\n\\newpage \n\n```{r mediantest}\nmedian_diff = two.boot(year1, year2, median, R=1000)\ncat(paste0(\"Difference in medians is \", abs(median_diff$t0), \".\"))\nboot.ci(median_diff, type = \"perc\") \n```\n\n\\  \n\nHere, we see that the effect size is a difference in medians of 1, but the confidence interval on that effect size goes from -1 to +1, i.e. is consistent with any score difference between *Neutral* and *Strongly Agree*. Since that CI includes 0, we can't say that the change from median of *Agree* to a median of *Strongly Agree* is statistically different, though again, sample size matters---one would probably like to try to intervene based on the one respondent who dropped down to 2 (*Disagree*) anyway. \n\n\n\n\n## *Neutral* scores matter\n\nYou might have noticed in some surveys that there is often no \"neutral\" or \"undecided\" category included in the middle of the scale, e.g., what's usually a 3 on a 5-point Likert scale. Sometimes it is placed at the end of the scale, and sometimes it is eliminated entirely. The reason for this is that those terms can sometimes be interpreted in a variety of ways; for example, with a question such as \"My pay is fair compared with other companies\", a *Neutral* response could indicate \"I'm neutral on this\", \"yes, I guess so\", \"I don't know\", \"it's neither fair nor unfair\", \"I don't want to answer\", \"I'm not sure what 'fair' means\", and any number of ideas that don't necessarily indicate a true neutral opinion.\n\nWhen a question has a response option where this type of ambiguity exists, a mean value will tend toward the that option because of this bias, unless of course the mean is already at that value. However, when *Neutral* is marked as 3, and when valid responses tend towards 4s and 5s, these ambiguous responses will drag down the average (and vice versa for responses heavy with 1s and 2s). Of course, you shouldn't use means anyway, as we've seen above, but many reports do---so understanding this effect is important toward interpreting the results in a useful way.  \n\nUse of a median is somewhat resistant to this problem, though you still won't know whether the middle values are valid responses or accidents of interpretation.  \n\nWhen you see an \"undecided\" or \"N/A\" response placed at the end of the scale or missing entirely, it is usually (but not always!) a sign that the survey creator understands this problem.  \n\nSometimes, of course, *Neutral* can be a completely reasonable and unambiguous response to a question. Context matters; while it's easiest for survey creators and scanning software to use the same scale for large numbers of questions, it is important that the analyst understand the extent to which *Neutral* and similar types of responses are a valid part of the measurement scale for each question.  \n\n\n\n\n## Similarities: correlation between ordinal-scale variables\n\nAlthough traditionally many analysts used non-parametric correlation like Spearman's or Kendall's, polychoric correlation is the proper tool to assess similarities between Likert scale results. (Polyserial correlation is used when one variable is numeric and the other is ordinal.) \n\n*Figure 5. Scatterplot of ordinal comparisons (jittered to show point density) between the questions \"My team works well together\" and \"I have the tools I need to do my job\".*  \n\n```{r polyc, fig.height=5}\npoly_c_both = polycor::polychor(both[,2], both[,3])\n\nggplot(both, aes(both[,2], both[,3], group=EmployeeType, color=EmployeeType)) +\n  geom_jitter(na.rm=TRUE, width = 0.15, height = 0.15, alpha=0.6, size=3) +\n  xlab(\"My team works well together\") + \n  ylab(\"I have the tools I need to do my job\") +\n  coord_equal()\n```\n\n\\  \n\nThe polychoric correlation coefficient between \"My team works well together\" and \"I have the tools I need to do my job\" is `r round(poly_c_both, 4)`. As expected, that suggests that there is no relationship between the responses to these two questions.  \n\n\\newpage  \n\n## Other ordinal-scale visualizations\n\n### Likert chart with response count histograms\n\n*Figure 6. A Likert chart for two different questions (e.g., as within a single year's survey), with a count histogram to show number of responses and non-answers for each question.*  \n\n```{r likert_viz1, fig.width=9.5}\nex_2_likert = likert(both[2:3])\n    \nplot(ex_2_likert, include.histogram = TRUE)\n```\n\n\\  \n\n### Uncentered Likert chart for multiple questions\n\n*Figure 7. An uncentered Likert chart for two different questions.*    \n\n```{r likert_viz2, fig.width=9.5}\nplot(ex_2_likert, centered = FALSE)\n```\n\n\\  \n\n### Heatmap\n\n*Figure 8. A heatmap of the response frequency for two different questions. While the use of means and SDs is inappropriate, this particular example directly illustrates why those values don't capture the response patterns in the data.*  \n\n```{r likert_viz3, fig.width=8}\nplot(ex_2_likert, type = \"heat\")\n```\n\n\\ \n\n```{r functions_edit_likert_plots}\n# FROM LIKERT GITHUB SITE\n\n# Not edited, needed for new barplot function\nlabel_wrap_mod <- function(value, width = 25) {\n  sapply(strwrap(as.character(value), width=width, simplify=FALSE), \n         paste, collapse=\"\\n\")\n}\n\nabs_formatter <- function(x) {\n\treturn(abs(x))\n}\n\n\n# Edited to correct color sequence on favorable side\nlikert.bar.plot = function (l, low.color = \"#D8B365\", high.color = \"#5AB4AC\", neutral.color = \"grey90\", \n    neutral.color.ramp = \"white\", colors = NULL, plot.percent.low = TRUE, \n    plot.percent.high = TRUE, plot.percent.neutral = TRUE, plot.percents = FALSE, \n    text.size = 3, text.color = \"black\", centered = TRUE, center = (l$nlevels - \n        1)/2 + 1, include.center = TRUE, ordered = TRUE, wrap = ifelse(is.null(l$grouping), \n        50, 100), wrap.grouping = 50, legend = \"Response\", legend.position = \"bottom\", \n    panel.arrange = \"v\", panel.strip.color = \"#F0F0F0\", group.order, \n    ...) \n{\n    if (center < 1.5 | center > (l$nlevels - 0.5) | center%%0.5 != \n        0) {\n        stop(paste0(\"Invalid center. Values can range from 1.5 to \", \n            (l$nlevels - 0.5), \" in increments of 0.5\"))\n    }\n    ymin <- 0\n    ymax <- 100\n    ybuffer <- 5\n    lowrange <- 1:floor(center - 0.5)\n    highrange <- ceiling(center + 0.5):l$nlevels\n    cols <- NULL\n    if (!is.null(colors) & length(colors) == l$nlevels) {\n        cols <- colors\n    }\n    else {\n        if (!is.null(colors) & length(colors) != l$nlevels) {\n            warning(\"The length of colors must be equal the number of levels.\")\n        }\n        ramp <- colorRamp(c(low.color, neutral.color.ramp))\n        ramp <- rgb(ramp(seq(0, 1, length = length(lowrange) + \n            1)), maxColorValue = 255)\n        bamp <- colorRamp(c(neutral.color.ramp, high.color))\n        bamp <- rgb(bamp(seq(0, 1, length = length(highrange) + \n            1)), maxColorValue = 255)\n        cols <- NULL\n        if (center%%1 != 0) {\n            cols <- c(ramp[1:(length(ramp) - 1)], bamp[2:length(bamp)])\n        }\n        else {\n            cols <- c(ramp[1:(length(ramp) - 1)], neutral.color, \n                bamp[2:length(bamp)])\n        }\n    }\n    lsum <- summary(l, center = center)\n    p <- NULL\n    if (!is.null(l$grouping)) {\n        lsum$Item <- label_wrap_mod(lsum$Item, width = wrap)\n        l$results$Item <- label_wrap_mod(l$results$Item, width = wrap)\n        lsum$Group <- label_wrap_mod(lsum$Group, width = wrap.grouping)\n        results <- l$results\n        results <- reshape2::melt(results, id = c(\"Group\", \"Item\"))\n        results$variable <- factor(results$variable, ordered = TRUE)\n        if (TRUE | is.null(l$items)) {\n            results$Item <- factor(as.character(results$Item), \n                levels = unique(results$Item), labels = label_wrap_mod(as.character(unique(results$Item)), \n                  width = wrap), ordered = TRUE)\n        }\n        else {\n            results$Item <- factor(results$Item, levels = label_wrap_mod(names(l$items), \n                width = wrap), ordered = TRUE)\n        }\n        ymin <- 0\n        if (centered) {\n            ymin <- -100\n            rows <- which(results$variable %in% names(l$results)[3:(length(lowrange) + \n                2)])\n            results[rows, \"value\"] <- -1 * results[rows, \"value\"]\n            if (center%%1 == 0) {\n                rows.mid <- which(results$variable %in% names(l$results)[center + \n                  2])\n                if (include.center) {\n                  tmp <- results[rows.mid, ]\n                  tmp$value <- tmp$value/2 * -1\n                  results[rows.mid, \"value\"] <- results[rows.mid, \n                    \"value\"]/2\n                  results <- rbind(results, tmp)\n                }\n                else {\n                  results <- results[-rows.mid, ]\n                }\n            }\n            results.low <- results[results$value < 0, ]\n            results.high <- results[results$value > 0, ]\n            p <- ggplot(results, aes(y = value, x = Group, group = variable)) + \n                geom_hline(yintercept = 0) + geom_bar(data = results.low[nrow(results.low):1, \n                ], aes(fill = variable), stat = \"identity\") + \n                geom_bar(data = results.high, aes(fill = variable, group=rev(variable)), # EDITED HERE\n                  stat = \"identity\")\n            names(cols) <- levels(results$variable)\n            p <- p + scale_fill_manual(legend, breaks = names(cols), \n                values = cols, drop = FALSE) +\n                theme_bw() +\n                theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) #EDITED HERE\n                \n        }\n        else {\n            ymin <- 0\n            p <- ggplot(results, aes(y = value, x = Group, group = variable))\n            p <- p + geom_bar(stat = \"identity\", aes(fill = variable)) + \n                scale_fill_manual(legend, values = cols, breaks = levels(results$variable), \n                  labels = levels(results$variable), drop = FALSE)\n        }\n        if (plot.percent.low) {\n            p <- p + geom_text(data = lsum, y = ymin, aes(x = Group, \n                label = paste0(round(low), \"%\"), group = Item), \n                size = text.size, hjust = 1, color = text.color)\n        }\n        if (plot.percent.high) {\n            p <- p + geom_text(data = lsum, aes(x = Group, y = 100, \n                label = paste0(round(high), \"%\"), group = Item), \n                size = text.size, hjust = -0.2, color = text.color)\n        }\n        if (plot.percent.neutral & l$nlevels%%2 == 1 & include.center) {\n            if (centered) {\n                p <- p + geom_text(data = lsum, y = 0, aes(x = Group, \n                  group = Item, label = paste0(round(neutral), \n                    \"%\")), size = text.size, hjust = 0.5, color = text.color)\n            }\n            else {\n                lsum$y <- lsum$low + (lsum$neutral/2)\n                p <- p + geom_text(data = lsum, aes(x = Group, \n                  y = y, group = Item, label = paste0(round(neutral), \n                    \"%\")), size = text.size, hjust = 0.5, color = text.color)\n            }\n        }\n        if (FALSE & plot.percents) {\n            warning(\"plot.percents is not currenlty supported for grouped analysis.\")\n        }\n        p <- p + coord_flip() + ylab(\"Percentage\") + xlab(\"\") + \n            theme(axis.ticks = element_blank(), strip.background = element_rect(fill = panel.strip.color, \n                color = panel.strip.color))\n        if (is.null(panel.arrange)) {\n            p <- p + facet_wrap(~Item)\n        }\n        else if (panel.arrange == \"v\") {\n            p <- p + facet_wrap(~Item, ncol = 1)\n        }\n        else if (panel.arrange == \"h\") {\n            p <- p + facet_wrap(~Item, nrow = 1)\n        }\n        if (!missing(group.order)) {\n            p <- p + scale_x_discrete(limits = rev(group.order), \n                drop = FALSE)\n        }\n    }\n    else {\n        factor.mapping <- NULL\n        if (!is.null(l$factors)) {\n            factor.mapping <- l$results[, 1:2]\n            names(factor.mapping)[2] <- \"Factor\"\n            results <- reshape2::melt(l$results[, -2], id.vars = \"Item\")\n        }\n        else {\n            results <- reshape2::melt(l$results, id.vars = \"Item\")\n        }\n        if (ordered & is.null(results$factor)) {\n            order <- lsum[order(lsum$high), \"Item\"]\n            results$Item <- factor(results$Item, levels = order)\n        }\n        ymin <- 0\n        if (centered) {\n            ymin <- -100\n            rows <- which(results$variable %in% names(l$results)[2:(length(lowrange) + \n                1)])\n            results[rows, \"value\"] <- -1 * results[rows, \"value\"]\n            if (center%%1 == 0) {\n                rows.mid <- which(results$variable %in% names(l$results)[center + \n                  1])\n                if (include.center) {\n                  tmp <- results[rows.mid, ]\n                  tmp$value <- tmp$value/2 * -1\n                  results[rows.mid, \"value\"] <- results[rows.mid, \n                    \"value\"]/2\n                  results <- rbind(results, tmp)\n                }\n                else {\n                  results <- results[-rows.mid, ]\n                }\n            }\n            if (!is.null(factor.mapping)) {\n                results$order <- 1:nrow(results)\n                results <- merge(results, factor.mapping, by = \"Item\", \n                  all.x = TRUE)\n                results <- results[order(results$order), ]\n                results$order <- NULL\n            }\n            results.low <- results[results$value < 0, ]\n            results.high <- results[results$value > 0, ]\n            p <- ggplot(results, aes(y = value, x = Item, group = Item)) + \n                geom_hline(yintercept = 0) + geom_bar(data = results.low[nrow(results.low):1, \n                ], aes(fill = variable), stat = \"identity\") + \n                geom_bar(data = results.high, aes(fill = variable, group=rev(variable)), # EDITED HERE\n                  stat = \"identity\")\n            names(cols) <- levels(results$variable)\n            p <- p + scale_fill_manual(legend, breaks = names(cols), \n                values = cols, drop = FALSE)\n        }\n        else {\n            if (!is.null(factor.mapping)) {\n                results$order <- 1:nrow(results)\n                results <- merge(results, factor.mapping, by = \"Item\", \n                  all.x = TRUE)\n                results <- results[order(results$order), ]\n                results$order <- NULL\n            }\n            p <- ggplot(results, aes(y = value, x = Item, group = Item))\n            p <- p + geom_bar(stat = \"identity\", aes(fill = variable))\n            p <- p + scale_fill_manual(legend, values = cols, \n                breaks = levels(results$variable), labels = levels(results$variable), \n                drop = FALSE) +\n                theme(axis.ticks = element_blank(), strip.background = element_rect(fill = panel.strip.color, \n                color = panel.strip.color))  #Editedhere\n        }\n        if (plot.percent.low) {\n            p <- p + geom_text(data = lsum, y = ymin, aes(x = Item, \n                label = paste0(round(low), \"%\")), size = text.size, \n                hjust = 1, color = text.color)\n        }\n        if (plot.percent.high) {\n            p <- p + geom_text(data = lsum, y = 100, aes(x = Item, \n                label = paste0(round(high), \"%\")), size = text.size, \n                hjust = -0.2, color = text.color)\n        }\n        if (plot.percent.neutral & l$nlevels%%2 == 1 & include.center) {\n            if (centered) {\n                p <- p + geom_text(data = lsum, y = 0, aes(x = Item, \n                  label = paste0(round(neutral), \"%\")), size = text.size, \n                  hjust = 0.5, color = text.color)\n            }\n            else {\n                lsum$y <- lsum$low + (lsum$neutral/2)\n                p <- p + geom_text(data = lsum, aes(x = Item, \n                  y = y, label = paste0(round(neutral), \"%\")), \n                  size = text.size, hjust = 0.5, color = text.color)\n            }\n        }\n        if (plot.percents) {\n            lpercentpos <- ddply(results[results$value > 0, ], \n                .(Item), transform, pos = cumsum(value) - 0.5 * \n                  value)\n            p <- p + geom_text(data = lpercentpos, aes(x = Item, \n                y = pos, label = paste0(round(value), \"%\")), \n                size = text.size, color = text.color)\n            lpercentneg <- results[results$value < 0, ]\n            if (nrow(lpercentneg) > 0) {\n                lpercentneg <- lpercentneg[nrow(lpercentneg):1, \n                  ]\n                lpercentneg$value <- abs(lpercentneg$value)\n                lpercentneg <- ddply(lpercentneg, .(Item), transform, \n                  pos = cumsum(value) - 0.5 * value)\n                lpercentneg$pos <- lpercentneg$pos * -1\n                p <- p + geom_text(data = lpercentneg, aes(x = Item, \n                  y = pos, label = paste0(round(abs(value)), \n                    \"%\")), size = text.size, color = text.color)\n            }\n        }\n        p <- p + coord_flip() + ylab(\"Percentage\") + xlab(\"\") + \n            theme(axis.ticks = element_blank())\n        if (!is.null(factor.mapping)) {\n        }\n        if (!missing(group.order)) {\n            p <- p + scale_x_discrete(limits = rev(group.order), \n                labels = label_wrap_mod(rev(group.order), width = wrap), \n                drop = FALSE)\n        }\n        else {\n            p <- p + scale_x_discrete(breaks = l$results$Item, \n                labels = label_wrap_mod(l$results$Item, width = wrap), \n                drop = FALSE)\n        }\n    }\n    p <- p + scale_y_continuous(labels = abs_formatter, limits = c(ymin - \n        ybuffer, ymax + ybuffer))\n    p <- p + theme(legend.position = legend.position)\n    attr(p, \"item.order\") <- levels(results$Item)\n    class(p) <- c(\"likert.bar.plot\", class(p))\n    return(p)\n}\n\n\n```\n\n### Likert chart with subgroups\n\n*Figure 9. Subgroups can sometimes reveal patterns not seen in aggregate data. For example, compare the overall results for \"My team works well together\" in Figure 5 (above) with the responses from the subgroups of MDs and RNs (below, bottom panel).*    \n\n```{r likert_viz4, fig.width=8, fig.height=4}\nboth_likert_2 = likert(both[, c(2:3), drop = FALSE], grouping = both$EmployeeType)\n\n# New ggplot 2.2 broke the grouping plots\n# plot(both_likert_2, include.histogram = TRUE)\n# Using edited function from above chunk\nlikert.bar.plot(both_likert_2, include.histogram = TRUE)\n\n```\n\n\\  \n\n### Density histograms\n\n*Figure 10. Density plots for the same data shown in Figure 8, above. While using a density plot on ordinal data is also statistically inappropriate, it can be a useful tool for an analyst. Bar histograms are difficult to overlay subgroups or different years for a direct comparsion, so must be separated into facets instead (e.g., Figure 1, above). Density plots are easier to overlay to show these comparisons, so while not appropriate for a report, they can be useful tools for an analyst during the exploration phase.*   \n\n```{r likert_viz5, fig.height=4}\nplot(both_likert_2, type = \"density\")\n\n## ggbeeswarm?\n```\n\n## Advanced analytics for ordinal scaled variables {#Advanced}\n\nWhile Mann-Whitney-Wilcoxon (sometimes known as the Mann-Whitney *U*-test) is the test most often used with differences between ordinal distributions, there are other options that can tell you whether a measured difference between groups is statistical different. \n\nThe old stand-by in this case is the $\\chi^2$ test, which is often best visualized with a mosaic plot.  \n\n```{r chisq}\n# Get rid of NAs\nboth2 = na.omit(both)\nnames(both2) = c(\"EmployeeType\", \"Teamwork\", \"Tools\")\nboth2$Teamwork = ordered(both2$Teamwork, levels = c(\"5\", \"4\", \"3\", \"2\", \"1\"))\n\n# Make a table object for chisq stuff\nboth2_tab = xtabs(~ both2$EmployeeType + both2$Teamwork)\n\n# Resampling version of chi-square\n# coin::chisq_test(both2_tab)\n\n```\n\n*Figure 11. Chi-square test and mosaic plot between Employee Type and responses to the \"My team works well together\" question.*   \n\n```{r mosaicplot, fig.height=4.25, fig.width=4.5}\n# Mosaic plot with Pearson residuals\nmosaicplot(both2_tab, shade = T, main=\"\", xlab=\"Employee Type\", ylab=\"My team works well together\")\n```\n\n```{r chisq_results}\n# Chi-square\nchisq.test(both2_tab, simulate.p.value = T)\n```\n\n\n```{r loglin}\n# Log-linear model instead of chi-square\n#logln_both = MASS::loglm(both2[,2] ~ both2[,1], both2_tab)\n# logln_both$params\n\n# Poisson glm\n# glm_both = glm(as.numeric(both2[,2]) ~ both2[,1], data=both2, family = poisson)\n# summary(glm_both)\n```\n\n\\ \n\nThe multinomial regression model is a more powerful (and more modern) version of the $\\chi^2$ test.  \n\n\\  \n\n*Figure 12. Multinomial regression between Employee Type and responses to the \"My team works well together\" question, with information-theoretic table for multi-model inference.*  \n\n```{r multnom}\n# Bring axis back to normal\nboth2$Teamwork = ordered(both2$Teamwork, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Multinomial regression\nmultnom_both = nnet::multinom(Teamwork ~ EmployeeType, data = both2, trace = FALSE)\nmultnom_both_1 = nnet::multinom(Teamwork ~ 1, data = both2, trace = FALSE)\n\n# multnom_both\n# exp(coef(multnom_both))\n# exp(confint(multnom_both))\n```\n\n```{r multnom_plot, fig.height=2.5}\n\n# New data for prediction\ndf_both = data.frame(EmployeeType = rep(c(\"MD\", \"RN\"), each = 5),  Teamwork = rep(c(1:5), 2))\n\n# Get probabilities\nmultnom_both_probs = cbind(df_both, predict(multnom_both, newdata = df_both, type = \"probs\", se = TRUE))\n\n# Clean up, ugh\nmultnom_both_probs = multnom_both_probs[,-2]\nmultnom_both_probs = unique(multnom_both_probs)\n\n# Make data frame for ggplot, probably should figure out tidyr\nmultnom_both_probs_df = reshape2::melt(multnom_both_probs, id.vars = \"EmployeeType\", variable.name = \"Teamwork\", value.name = \"probability\")\n\n# Plot multinomial regression probs for Employee Type\nggplot(multnom_both_probs_df, aes(x = Teamwork, y = probability, color = EmployeeType, group = EmployeeType)) +\n  geom_line() + \n  geom_point() +\n  xlab(\"My team works well together\")\n```\n\n```{r multnom_model_comps}\n# AICc table\nmod_set = list()\n    mod_set[[1]] = multnom_both\n    mod_set[[2]] = multnom_both_1\n\nkable(aictab(mod_set, modnames = c(\"Employee Type\", \"Null Model\")))\n```\n\n\\  \n\nIf you can meet the assumptions, the proportional-odds regression is more powerful than the multinomial model, as it can take into account the ordered nature of the ordinal scale.  \n\n*Figure 13. Proportional odds logistic regression between Employee Type and responses to the \"My team works well together\" question, with information-theoretic table for multi-model inference.*  \n\n```{r prop_odds, fig.height = 2.5}\n# Data frame for proportional odds regression\nTeamwork_tab_long = both2[,1:2] %>%\n  group_by(EmployeeType, Teamwork) %>%\n  summarize(Count = n())\n\n# Proportional odds regression with polr\npolr_both = MASS::polr(Teamwork ~ EmployeeType, data = Teamwork_tab_long, weight = Count)\n#polr_both_1 = MASS::polr(Teamwork ~ 1, data = Teamwork_tab_long, weight = Count)\n#polr_both\n#exp(coef(polr_both))\n#exp(confint(polr_both))\n\n\n\n# New data for prediction, same as multinom\ndf_both = data.frame(EmployeeType = rep(c(\"MD\", \"RN\"), each = 5),  Teamwork = rep(c(1:5), 2))\n\n# Get probabilities\npolr_both_probs = cbind(df_both, predict(polr_both, newdata = df_both, type = \"probs\", se = TRUE))\n\n# Clean up, ugh\npolr_both_probs = polr_both_probs[,-2]\npolr_both_probs = unique(polr_both_probs)\n\n# Make data frame for ggplot, probably should figure out tidyr\npolr_both_probs_df = reshape2::melt(polr_both_probs, id.vars = \"EmployeeType\", variable.name = \"Teamwork\", value.name = \"probability\")\n\n# Plot prop odds regression probs for Employee Type\nggplot(polr_both_probs_df, aes(x = Teamwork, y = probability, color = EmployeeType, group = EmployeeType)) +\n  geom_line() + \n  geom_point() +\n  xlab(\"My team works well together\")\n```\n\n\n```{r polr_model_comps}\n\ncountsToCases = function(x, countcol = \"Count\") {\n    # Get the row indices to pull from x\n    idx = rep.int(seq_len(nrow(x)), x[[countcol]])\n    # Drop count column\n    x[[countcol]] = NULL\n    # Get the rows from x\n    x[idx, ]\n}\n\n# Make a data table\nTeamwork_tab_long$Teamwork_Group = as.numeric(Teamwork_tab_long$Teamwork) \nTeamwork_tab_long$Teamwork = ordered(Teamwork_tab_long$Teamwork) \ntab_df = data.frame(countsToCases(Teamwork_tab_long, countcol=\"Count\"))\n\n# Need to better understand diffs between polr and clm\n# Coefs/thresholds are exactly the same, though\nfm1 = ordinal::clm(Teamwork ~ EmployeeType, data=tab_df)\nfm2 = ordinal::clm(Teamwork ~ EmployeeType, data=tab_df, threshold=\"equidistant\")\n\n# Null model\nfm3 = ordinal::clm(Teamwork ~ 1, data=tab_df)\n\n# AICc table\nmod_set = list()\n    mod_set[[1]] = fm1\n    mod_set[[2]] = fm3\n\nkable(aictab(mod_set, modnames = c(\"Employee Type\", \"Null Model\")))\n\n```\n\n```{r polr_assumptions}\n#The assumption of proportional-odds is ok \n# Worth showing?\n\npolr_assumptions = anova(fm1, fm2)\n```\n\nIf the concepts or ideas in this section are confusing, it's probably worth consulting a statistician for help evaluating your data with these tools.  \n\n\\newpage  \n\n# Appendix: Measurement Levels & Appropriate Summary Statistics {#Appendix}\n\n\\  \n\n------------------------------------------------------------------------------------------\nStatistic /\\               Categorical\\   Ranked\\     Discrete/Counts\\   Continuous\\\nParameter                  *Nominal*      *Ordinal*   *Interval/Ratio*   *Interval/Ratio* \n------------------------- -------------- ----------- ------------------ ------------------\nData set size (n)\\         Y\\              Y\\          Y\\                  Y\\ \nPercent / Frequency\\       Y\\              Y\\          Y\\                  Y\\ \nCount or rate\\             Y\\              Y\\          Y\\                  Y\\ \nCategories (levels)\\       Y\\              Y\\          Y\\                  Y\\ \nMode\\                      Y\\              Y\\          Y\\                  Y\\ \nMedian\\                    *No*\\              Y\\          Y\\                  Y\\ \nInterquartile range\\       *No*\\              Y\\          Y\\                  Y\\ \nMedian absolute deviation\\ *No*\\              Y\\          Y\\                  Y\\ \nRange\\                     *No*\\              Y\\          Y\\                  Y\\ \nMinimum/maximum value\\     *No*\\              Y\\          Y\\                  Y\\ \nQuantiles\\                 *No*\\              Y\\          Y\\                  Y\\ \nMean (average)\\            *No*\\              *No*\\          Y\\                 Y\\ \nStandard deviation\\        *No*\\              *No*\\          Y\\*\\               Y\\*\\ \nCoefficient of variation\\  *No*\\              *No*\\          Y\\*\\               Y\\*\\ \n------------------------------------------------------------------------------------------\n<small>\\* You must use the correct distribution (proper mean-variance relationship) to ensure you get the correct standard deviation; most software defaults to calculating the standard deviation for a normally-distributed sample, which could be incorrect for certain kinds of count, rate, or proportion data, for example.</small>  \n\n",
    "created" : 1483411196555.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "4|27|57|0|\n",
    "hash" : "321939774",
    "id" : "DCFBAC8E",
    "lastKnownWriteTime" : 1483410963,
    "last_content_update" : 1483412165020,
    "path" : "~/Documents/R/SCH_R_Training/Ordinal_Guidelines_pdf.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}